# Artificial Intelligence for Translation, Interpreting and Specialised Communication – Technical Curriculum
<img src = "https://github.com/ITMK/AI_Literacy/blob/main/images/GenAI_ITMK.jpg?raw=true" width="20%" height="20%">

## The Curriculum

### 1. Vector embeddings – Representing linguistic data (and other modalities) as numbers
   1.1 Word embeddings
   
   1.2 Sentence embeddings
   
   1.3 Decontextualised vs. contextualised embeddings
   
   1.4 Audio embeddings
   
   1.5 Image embeddings

### 2. Technical foundations of neural networks
   2.1 Basic building blocks and information flow through a network (basic Python implementation)
   
   2.2 Forward pass – Processing input information to produce an output
   
   2.3 Backward pass – Training a neural network

### 3. Tokenisation – Reducing the vocabulary size of neural networks
   3.1 Word-based tokenisation
   
   3.2 Character-based tokenisation
   
   3.3 Subword-based tokenisation

### 4. Transformer neural networks
   4.1 Encoder-Decoder (sequence2sequence) models (e.g. BART)
   
   4.2 Encoder-only models (e.g. BERT)
   
   4.3 Decoder-only models (e.g. GPT)
   
   4.4 Self-Attention
   
   4.5 Output generation (Softmax and Beam Search)

### 5. Fine-tuning a Transformer neural network for translation
   5.1 Pre-training vs. fine-tuning vs. reinforcement learning
   
   5.2 Fine-tuning pipeline

### 6. Current developments in language-oriented AI
   6.1 Multimodal large language models
   
   6.2 Large reasoning models

   6.3 Large action models

   6.4. Small language models

   6.5 Retrieval-augmented generation

   6.6 Knowledge graphs
