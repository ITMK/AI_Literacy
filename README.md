# Artificial Intelligence for Translation, Interpreting and Specialised Communication – Technical Curriculum (Work in Progress)
<img src = "https://github.com/ITMK/AI_Literacy/blob/main/images/GenAI_ITMK.jpg?raw=true">

Author: [Ralph Krüger](https://www.th-koeln.de/en/person/ralph.krueger/)

License: [CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)](https://creativecommons.org/licenses/by-sa/4.0/)

## The Curriculum

### Prelude – Python for beginners
-    This [Colab notebook](https://colab.research.google.com/drive/10G_zfSUssTVqLTkWl5NpwJvQS3Mbam_B?usp=sharing) written as part of the [LT-LiDER project](http://lt-lider.eu/) provides a concise introduction to Python             programming for novices.

-    The paper [Bridging the Digital Divide – Making Python and the Python Ecosystem Accessible to Translators](http://dx.doi.org/10.13140/RG.2.2.13246.40006) by Krüger/Álvarez Vidal/Hackenbuchner (forthcoming) provides an         introduction to Colab notebooks as online Python programming environments and discusses strategies for using large language models as coding assistants to programming novices. The paper was also written as part of the         LT-LiDER project.

### 1. Vector embeddings – Representing linguistic data (and other modalities) as numbers
In this [Colab notebook on vector embeddings](https://colab.research.google.com/drive/1-R3ftZceMORC-fv9J6mecjrg-efLELz6?usp=sharing), we cover the following topics:

   - Word embeddings
   
   - Decontextualised vs. contextualised embeddings
   
   - Sentence embeddings

   - Multilingual embeddings
   
   - Outook: Embeddings of other modalities (e.g., image and audio embeddings) in multimodal LLMs

### 2. Technical foundations of neural networks – Building blocks, forward and backward pass
In this [Colab notebook on the technical foundations of neural networks](https://colab.research.google.com/drive/1AaVPBOTa3K8WQ6USrqx1ZR556RpF8-RC?usp=sharing), we cover the following topics:

   - Basic building blocks and information flow through a network
   
   - Forward pass – Processing input information to produce an output
   
   - Backward pass – Training a neural network

### 3. Tokenisation – Reducing the vocabulary size of neural networks
In this [Colab notebook on tokenisation](https://colab.research.google.com/drive/190K0BZZf9ChCNFd7ADbQ5JL24KWPkLU0?usp=sharing), we cover the following topics:

   - Word-based tokenisation
   
   - Character-based tokenisation
   
   - Subword-based tokenisation

   - From subword tokenisation to embeddings

### 4. Transformer neural networks
In this [Colab notebook on transformer neural networks](https://colab.research.google.com/drive/1dN6wA7li0Zai_AyPFS4igMS-NvTZPkAS?usp=sharing), we cover the following topics:
   
   - Encoder-Decoder (sequence2sequence) models (e.g. BART)
   
   - Encoder-only models (e.g. BERT)
   
   - Decoder-only models (e.g. GPT)
   
   - Self-Attention
   
   - Output generation (Softmax and Beam Search)

### 5. Fine-tuning a Transformer neural network for translation
   - Pre-training vs. fine-tuning vs. reinforcement learning
   
   - Fine-tuning pipeline

### 6. Current developments in language-oriented AI
   - Multimodal large language models
   
   - Large reasoning models

   - Large action models

   - Small language models

   - Retrieval-augmented generation

   - Knowledge graphs

### Annex – A glossary of language-oriented AI
