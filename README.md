# Artificial Intelligence for Translation, Interpreting and Specialised Communication – Technical Curriculum (Work in Progress)
<img src = "https://github.com/ITMK/AI_Literacy/blob/main/images/GenAI_ITMK.jpg?raw=true">

## The Curriculum

### 1. Vector embeddings – Representing linguistic data (and other modalities) as numbers
In this [Colab notebook on vector embeddings](https://colab.research.google.com/drive/1-R3ftZceMORC-fv9J6mecjrg-efLELz6?usp=sharing), we cover the following topics:

   - Word embeddings
   
   - Decontextualised vs. contextualised embeddings
   
   - Sentence embeddings

   - Multilingual embeddings
   
   - Audio embeddings
   
   - Image embeddings

### 2. Technical foundations of neural networks – Building blocks, forward and backward pass
In this [Colab notebook on the technical foundations of neural networks](https://colab.research.google.com/drive/1AaVPBOTa3K8WQ6USrqx1ZR556RpF8-RC?usp=sharing), we cover the following topics:

   - Basic building blocks and information flow through a network
   
   - Forward pass – Processing input information to produce an output
   
   - Backward pass – Training a neural network

### 3. Tokenisation – Reducing the vocabulary size of neural networks
In this [Colab notebook on tokenisation](https://colab.research.google.com/drive/190K0BZZf9ChCNFd7ADbQ5JL24KWPkLU0?usp=sharing), we cover the following topics:

   - Word-based tokenisation
   
   - Character-based tokenisation
   
   - Subword-based tokenisation

   - From subword tokenisation to embeddings

### 4. Transformer neural networks
In this [Colab notebook on transformer neural networks](https://colab.research.google.com/drive/1dN6wA7li0Zai_AyPFS4igMS-NvTZPkAS?usp=sharing), we cover the following topics:
   
   - Encoder-Decoder (sequence2sequence) models (e.g. BART)
   
   - Encoder-only models (e.g. BERT)
   
   - Decoder-only models (e.g. GPT)
   
   - Self-Attention
   
   - Output generation (Softmax and Beam Search)

### 5. Fine-tuning a Transformer neural network for translation
   - Pre-training vs. fine-tuning vs. reinforcement learning
   
   - Fine-tuning pipeline

### 6. Current developments in language-oriented AI
   - Multimodal large language models
   
   - Large reasoning models

   - Large action models

   - Small language models

   - Retrieval-augmented generation

   - Knowledge graphs

### Annex – A glossary of language-oriented AI
