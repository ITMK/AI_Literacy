# Artificial Intelligence for Translation, Interpreting and Specialised Communication – Technical Curriculum
<img src = "https://github.com/ITMK/AI_Literacy/blob/main/images/GenAI_ITMK.jpg?raw=true">

Author: [Ralph Krüger](https://www.th-koeln.de/en/person/ralph.krueger/)

License: [CC BY-SA 4.0 (Attribution-ShareAlike 4.0 International)](https://creativecommons.org/licenses/by-sa/4.0/)

## The Curriculum

### Prelude – Python for beginners
-    This [Colab notebook](https://colab.research.google.com/drive/10G_zfSUssTVqLTkWl5NpwJvQS3Mbam_B?usp=sharing) written as part of the [LT-LiDER project](http://lt-lider.eu/) provides a concise introduction to Python             programming for novices.

-    The paper [Bridging the Digital Divide – Making Python and the Python Ecosystem Accessible to Translators](http://dx.doi.org/10.13140/RG.2.2.13246.40006) by Krüger/Álvarez Vidal/Hackenbuchner (forthcoming) provides an         introduction to Colab notebooks as online Python programming environments and discusses strategies for using large language models as coding assistants to programming novices. The paper was also written as part of the         LT-LiDER project.

-    [Artificial Intelligence Literacy for the Language and Translation Industry - Conceptual Foundations, Operationalisation, Acquisition, Measurement](https://doi.org/10.13140/RG.2.2.31720.99843)

### 1. Vector embeddings – Representing linguistic data (and other modalities) as numbers
In this [Colab notebook on vector embeddings](https://colab.research.google.com/drive/1-R3ftZceMORC-fv9J6mecjrg-efLELz6?usp=sharing), we cover the following topics:

   - Word embeddings
   
   - Decontextualised vs. contextualised embeddings
   
   - Sentence embeddings

   - Multilingual embeddings
   
   - Outook: Embeddings of other modalities (e.g., image and audio embeddings) in multimodal LLMs

### 2. Technical foundations of neural networks – Building blocks, forward and backward pass
In this [Colab notebook on the technical foundations of neural networks](https://colab.research.google.com/drive/1AaVPBOTa3K8WQ6USrqx1ZR556RpF8-RC?usp=sharing), we cover the following topics:

   - Basic building blocks and information flow through a network
   
   - Forward pass – Processing input information to produce an output
   
   - Backward pass – Training a neural network

### 3. Tokenisation – Reducing the vocabulary size of neural networks
In this [Colab notebook on tokenisation](https://colab.research.google.com/drive/190K0BZZf9ChCNFd7ADbQ5JL24KWPkLU0?usp=sharing), we cover the following topics:

   - Word-based tokenisation
   
   - Character-based tokenisation
   
   - Subword-based tokenisation

   - From subword tokenisation to embeddings

### 4. Transformer neural networks (work in progress)
In this [Colab notebook on transformer neural networks](https://colab.research.google.com/drive/1dN6wA7li0Zai_AyPFS4igMS-NvTZPkAS?usp=sharing), we cover the following topics:
   
   - Encoder-Decoder (sequence2sequence) models (e.g. BART)
   
   - Encoder-only models (e.g. BERT)
   
   - Decoder-only models (e.g. GPT)
   
   - The inner workings of the transformer (self-attention, output generation, etc.)

### 5. Training and adaptation strategies for language models (coming soon)
In this [Colab notebook on traning and adaptation strategies for language models](https://colab.research.google.com/drive/1NCnI54IHJNwcpSovfsRC8IMGkEPUIFQs?usp=sharing), we cover the following topics: 
   - Pre-training
   
   - Fine-tuning
   
   - Reinforcement learning
   
   - In-context learning

   -  Hands-on session: Fine-tuning a transformer language model for translation

### 6. Current developments in language-oriented AI (coming soon)
In this [Colab notebook on current developments in language-oriented AI](https://colab.research.google.com/drive/1TUbbKLIPqmdN_xUQfoyb5ujEgxR_8jVp?usp=sharing), we cover the following topics:
   - Multimodal large language models (MM-LLMs)
   
   - Large reasoning models (LRMs)

   - Large action models (LAMs)

   - Small language models (SLMs)

   - Knowledge-enhanced LLMs (RAG + Knowledge Graphs)

### Annex – A glossary of language-oriented AI (work in progress)
This [glossary](https://th-koeln-1.gitbook.io/th-koeln-docs) explains key terms in the field of language-oriented AI that are mentioned at various points throughout this curriculum. 
